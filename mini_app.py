import streamlit as st
import re
import time
import requests
import pandas as pd
from bs4 import BeautifulSoup
from openai import OpenAI
from dotenv import load_dotenv
import os

#ìŠ¤ê¾¸
os.makedirs(".streamlit", exist_ok=True)
with open(".streamlit/config.toml", "w") as f:
    f.write("""[theme]
primaryColor = "#BCD0E2"
backgroundColor = "#FFFFFF"
secondaryBackgroundColor = "#F0F2F6"
textColor = "#222B52"
font = "sans serif"
""")
    
# ì œëª©
st.markdown("""
    <div style='text-align: center;'>
        <h1 style='font-size:45px; color: #262F60;'>Google Scholar ë…¼ë¬¸ ê²€ìƒ‰</h1>
        <p style='font-size:23px; color: #262F60;'>SJR ë­í‚¹ & ì¸ìš©íšŸìˆ˜ ê¸°ë°˜ ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼âœ¨</p>
    </div>
""", unsafe_allow_html=True)
st.set_page_config(layout="wide")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.write("âœ¨ğŸªğŸŒŸğŸŒ™ğŸŒ©â˜€")
    st.write("ê±°ì¸ì˜ ì–´ê¹¨ì—")
    st.write("ì˜¬ë¼ì„œì„œ")
    st.write("ë” ë„“ì€ ì„¸ìƒì„")
    st.write("ë°”ë¼ë³´ë¼")
    st.write("-- ì•„ì´ì‘ ë‰´í„´ --")
    st.write("âœ¨ğŸªğŸŒŸğŸŒ™ğŸŒ©â˜€")

    side_options = [5, 6, 7, 8, 9]
    num_sides = st.radio("í™”ë©´ ë¹„ìœ¨", side_options)
    font_scale = st.slider("ê¸€ì í¬ê¸°:", 1, 10, value=2)

# ë ˆì´ì•„ì›ƒ
col1, col2 = st.columns([num_sides, 10 - num_sides])

#ì „ì²´ í°íŠ¸ ì‚¬ì´ì¦ˆ ì¡°ì ˆ
font_style = f"""
    <style>
    .custom-text {{
        font-size: {font_scale * 5}px;
        line-height: 1.6;
    }}
    input {{
        font-size: {font_scale * 5}px;
    }}
    </style>
"""
st.markdown(font_style, unsafe_allow_html=True)

# ì»¬ëŸ¼1
with col1:
    st.markdown('<div class="custom-text"> </div>', unsafe_allow_html=True)
    
    st.markdown("""
    <style>
        input[aria-label="êµ¬ê¸€ì´ ë§Œë“  ì¿ ê¸°, ë‚˜ë¥¼ ìœ„í•´ êµ¬ì› ì§€ ğŸª"] {
            font-size: 7px;
            height: 18px;
        }
        div:has(>div>input[aria-label="êµ¬ê¸€ì´ ë§Œë“  ì¿ ê¸°, ë‚˜ë¥¼ ìœ„í•´ êµ¬ì› ì§€ ğŸª"]) {
            height: 1rem
        }
    </style>
    """, unsafe_allow_html=True)

    # ì‚¬ìš©ì ì¿ í‚¤ ì…ë ¥
    cookies = st.text_input("êµ¬ê¸€ì´ ë§Œë“  ì¿ ê¸°, ë‚˜ë¥¼ ìœ„í•´ êµ¬ì› ì§€ ğŸª")
    
    st.link_button(
        "google scholar",
        "https://scholar.google.co.kr/schhp?hl=ko&as_sdt=0,5",
         help=None,
         type="secondary",
         use_container_width=False)
    
    # ë°ì´í„° í”„ë ˆì„
    def google_scholar(query: str, num_pages: int) -> pd.DataFrame :
        q = query.replace(' ', '+')
        result = []
        for page in range(num_pages):
            try:
                start = page * 10
                res=requests.get(f'https://scholar.google.com/scholar?start={start}&q={q}&hl=en&as_sdt=0,5', headers={
                    'user-agent': 'Mozilla/5.0',
                    'cookie': cookies})
                time.sleep(3)

                soup = BeautifulSoup(res.text, 'html.parser')
                feeds = soup.select('div#gs_res_ccl_mid > div.gs_r.gs_or.gs_scl')
            
                for feed in feeds:
                    try:
                        info = feed.attrs.get('data-cid') or feed.attrs.get('data-aid')
                        res2 = requests.get(f'https://scholar.google.com/scholar?q=info:{info}:scholar.google.com/&output=cite&scirp=0&hl=en', headers={
                            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
                            'cookie': cookies})
                        time.sleep(2)

                        soup2 = BeautifulSoup(res2.text, 'html.parser')
                        journer_tag = soup2.select_one('div.gs_citr > i')
                        journers = journer_tag.text if journer_tag else ''

                        day = feed.select_one('div.gs_a')
                        publish = re.search(r'\d{4}', day.text).group() if day and re.search(r'\d{4}', day.text) else ''

                        title_tag = feed.select_one('h3.gs_rt')
                        titles = re.sub(r'\[.+?\]', '', title_tag.text) if title_tag else ''

                        short_tag = feed.select_one('div.gs_rs')
                        shorts = short_tag.text if short_tag else ''

                        cite_tags = feed.select('.gs_fl.gs_flb > a')
                        cites = cite_tags[2].text.split()[-1] if len(cite_tags) > 2 else '0'

                        link_tag = title_tag.find('a') if title_tag else None
                        links = link_tag['href'] if link_tag else ''

                        temp = {
                            'ë…¼ë¬¸ëª…': titles,
                            'ìš”ì•½': shorts,
                            'ì¸ìš©ìˆ˜': int(cites),
                            'ì €ë„ëª…': journers,
                            'ë°œí–‰ë…„ë„': publish,
                            'ì£¼ì†Œ': links
                        }
                        result.append(temp)

                    except Exception as e:
                        print(f"ë¹„ìƒ ! ê°œë³„ ë…¼ë¬¸ í¬ë¡¤ë§ ì¤‘ ì—ëŸ¬: {e}")
                        continue

            except Exception as e:
                print(f"ë¹„ìƒ ! {page}í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘ ì—ëŸ¬ : {e}")
                continue

        return pd.DataFrame(result)

    #ë§¤ì¹­
    def matches (search_df: pd.DataFrame, journal_df: pd.DataFrame) -> pd.DataFrame : 
        
        sjr_list = []
        
        for v1 in search_df['ì €ë„ëª…']:
            for v2 in journal_df.iloc: # í–‰ì„ ê°€ì ¸ì˜´
                
                new_v1 = re.sub(r'[^a-z0-9]', '', v1.lower().strip())
                new_v2 = re.sub(r'[^a-z0-9]', '', v2['ì €ë„ëª…'].lower().strip())
                
                if new_v1[:40] == new_v2[:40] or new_v1[-40:] == new_v2[-40:] :
                    print (f" ì°¾ì•˜ë‹¤! {v1} ìŒë‘¥ì´ {v2['ì €ë„ëª…']}")
                    sjr_list.append(v2['SJR'])
                    break
            else:
                sjr_list.append(0)

        search_df = search_df.copy()
        search_df['SJR'] = sjr_list

        return search_df
            
    query = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”", value="Artificial intelligence")
    pages = st.number_input("ëª‡ í˜ì´ì§€ê¹Œì§€ ê²€ìƒ‰í• ê¹Œìš”?", min_value=1, max_value=50, value=1, step=1)

# ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì‹œ ë°ì´í„° ì €ì¥
    st.markdown("""
        <style>
        .small-button {
            display: inline-block;
            padding: 0.3rem 0.6rem;
            margin-right: 0.3rem;
            font-size: 0.8rem;
            border-radius: 6px;
            background-color: #BCD0E2;
            color: black;
            text-decoration: none;
            border: 1px solid #999;
            }
        .small-button:hover {
            background-color: #a8c4d8;
        }
        </style>
        """, unsafe_allow_html=True)
    
    search_clicked = False
    clear_clicked = False

    # ë²„íŠ¼ UI
    btn_cols = st.columns([1, 1, 1])
    with btn_cols[0]:
        if st.button("ê²€ìƒ‰ ì‹œì‘", key="search_btn"):
            search_clicked = True
    with btn_cols[1]:
        if st.button("ê²€ìƒ‰ ì´ˆê¸°í™”", key="clear_btn"):
            clear_clicked = True
    with btn_cols[2]:
        st.link_button("sci-hub", "https://sci-hub.se/", use_container_width=True)

    
# ê²€ìƒ‰ ì‹œì‘ ì‹œ
if search_clicked:
    df = google_scholar(query, pages)
    st.success(f"ì´ {len(df)}ê°œì˜ ë…¼ë¬¸ì´ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")

    journal_df = pd.read_csv('./jounal_score.csv')
    df_with_sjr = matches(df, journal_df)

    df_sorted = df_with_sjr.sort_values(by=["SJR", "ì¸ìš©ìˆ˜"], ascending=[False, False]).reset_index(drop=True)

    st.session_state.df_sorted = df_sorted
    st.session_state.selected_index = 0  # ì„ íƒ ì´ˆê¸°í™”

# ì´ˆê¸°í™” ë²„íŠ¼ ë™ì‘
if clear_clicked:
    for key in ['df_sorted', 'selected_index']:
        st.session_state.pop(key, None)
    st.session_state.refresh_token = time.time()  

# ê²€ìƒ‰ ê²°ê³¼ê°€ ì„¸ì…˜ì— ìˆì„ ê²½ìš°
if "df_sorted" in st.session_state:
    df_sorted = st.session_state.df_sorted

    # ì„ íƒ ì¸ë±ìŠ¤ ì´ˆê¸°í™”
    if "selected_index" not in st.session_state:
        st.session_state.selected_index = 0

    # ë…¼ë¬¸ ì œëª© ë¦¬ìŠ¤íŠ¸ ìƒì„±
    paper_titles = [f"{row['ë…¼ë¬¸ëª…']} ({row['ë°œí–‰ë…„ë„']})" for _, row in df_sorted.iterrows()]

    # selectbox í•­ìƒ ì‹¤í–‰
    selected_title = st.selectbox("ë…¼ë¬¸ì„ ì„ íƒí•˜ì„¸ìš”", paper_titles, index=st.session_state.selected_index)
    st.session_state.selected_index = paper_titles.index(selected_title)

    # ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸ í‘œì‹œ
    st.dataframe(df_sorted[['ë…¼ë¬¸ëª…', 'ì €ë„ëª…', 'ë°œí–‰ë…„ë„','ì¸ìš©ìˆ˜','SJR']], use_container_width=True)

    #ìƒì„¸ì •ë³´
    with col2:
        selected_row = df_sorted.iloc[st.session_state.selected_index]
        st.markdown('<div class="custom-text"><h3>ë…¼ë¬¸ ì„¸ë¶€ì •ë³´</h3></div>', unsafe_allow_html=True)
        st.markdown(f"<div class='custom-text'><b>ì €ë„ëª…:</b> {selected_row['ì €ë„ëª…']}</div>", unsafe_allow_html=True)
        st.markdown(f"<div class='custom-text'><b>ë°œí–‰ë…„ë„:</b> {selected_row['ë°œí–‰ë…„ë„']}</div>", unsafe_allow_html=True)
        st.markdown(f"<div class='custom-text'><b>ìš”ì•½:</b> {selected_row['ìš”ì•½']}</div>", unsafe_allow_html=True)
        st.markdown(f"<div class='custom-text'><b>ì¸ìš©ìˆ˜:</b> {selected_row['ì¸ìš©ìˆ˜']}</div>", unsafe_allow_html=True)
        st.markdown(f"<div class='custom-text'><b>SJR:</b> {selected_row['SJR']}</div>", unsafe_allow_html=True)
        st.markdown(f"<div class='custom-text'><a href='{selected_row['ì£¼ì†Œ']}' target='_blank'>DOI</a></div>", unsafe_allow_html=True)


#ì œëª©
st.markdown("""
    <div style='text-align: center;'>
        <h1 style='font-size:45px; color: #262F60;'>ë…¼ë¬¸ ì¶”ì²œ GPT ì±—ë´‡ ğŸ’­</h1>
    </div>
""", unsafe_allow_html=True)

#gpt api
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)

user_input = st.text_input("ê²€ìƒ‰ì–´:")

if "messages" not in st.session_state:
    st.session_state.messages = []

gpt_col1, gpt_col2 = st.columns([1, 1], gap="small")
with gpt_col1:
    gpt_clicked = st.button("ì•Œë ¤ì¤˜ GPT", use_container_width=True)

with gpt_col2:
    clear_gpt = st.button("ë‚´ìš© ì´ˆê¸°í™”", use_container_width=True)

# GPT ë²„íŠ¼ í´ë¦­ ì‹œ ì²˜ë¦¬
if gpt_clicked and user_input:
    keyword = user_input.strip()
    st.session_state.messages.append({"role": "user", "content": user_input})

    prompt_messages = [
        {"role": "user", "content":
        f"""You're a helpful assistant for recommending papers based on the user's search intent.
            You should analyze the papers found to accurately figure out what the user wants.
            The accuracy of the search results is determined by the frequency of search terms in user's search server paper summaries.
            You should recommend papers sorted by journal impact factor score and citation count.

            Use a scoring system: 
            +8 for high citation, 
            +9 points for high SJR, 
            +8 for {keyword} relevance.

            If there are no direct matches, recommend papers that are contextually relevant.
            Show summaries with each DOI source, each journal name and published years when recommending papers.
            Suggest more than 5 papers.
        
            Tone: Friendly and professional."""},

        {"role": "user", "content": f"""Your searching keyword is {keyword}."""}
            ]

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=prompt_messages,
        max_tokens=1000,
        temperature=0.1,
        stream=False
    )

    gpt_reply = response.choices[0].message.content.strip()
    st.session_state.messages.append({"role": "assistant", "content": gpt_reply})

# ì´ˆê¸°í™” ë²„íŠ¼ ì²˜ë¦¬
if clear_gpt:
    st.session_state.messages = []

#gpt ì¶œë ¥
for msg in st.session_state.messages:
    st.markdown(f"<div style='font-size:{font_scale * 5}px'><b>{msg['role'].capitalize()}</b>: {msg['content']}</div>", unsafe_allow_html=True)
